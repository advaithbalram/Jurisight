{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.2753128555176336,
  "eval_steps": 500,
  "global_step": 4000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005688282138794084,
      "grad_norm": 1.1840522289276123,
      "learning_rate": 4.9905195297686766e-05,
      "loss": 3.6424,
      "step": 10
    },
    {
      "epoch": 0.011376564277588168,
      "grad_norm": 1.1066473722457886,
      "learning_rate": 4.9810390595373535e-05,
      "loss": 3.8112,
      "step": 20
    },
    {
      "epoch": 0.017064846416382253,
      "grad_norm": 1.4943959712982178,
      "learning_rate": 4.97155858930603e-05,
      "loss": 3.7862,
      "step": 30
    },
    {
      "epoch": 0.022753128555176336,
      "grad_norm": 1.267715573310852,
      "learning_rate": 4.962078119074706e-05,
      "loss": 3.2721,
      "step": 40
    },
    {
      "epoch": 0.02844141069397042,
      "grad_norm": 1.2657086849212646,
      "learning_rate": 4.952597648843383e-05,
      "loss": 3.3949,
      "step": 50
    },
    {
      "epoch": 0.034129692832764506,
      "grad_norm": 1.1803443431854248,
      "learning_rate": 4.9431171786120595e-05,
      "loss": 3.2137,
      "step": 60
    },
    {
      "epoch": 0.03981797497155859,
      "grad_norm": 1.294848084449768,
      "learning_rate": 4.933636708380736e-05,
      "loss": 3.1255,
      "step": 70
    },
    {
      "epoch": 0.04550625711035267,
      "grad_norm": 1.0492851734161377,
      "learning_rate": 4.924156238149412e-05,
      "loss": 3.0084,
      "step": 80
    },
    {
      "epoch": 0.051194539249146756,
      "grad_norm": 1.2727558612823486,
      "learning_rate": 4.914675767918089e-05,
      "loss": 3.0637,
      "step": 90
    },
    {
      "epoch": 0.05688282138794084,
      "grad_norm": 1.1558613777160645,
      "learning_rate": 4.9051952976867654e-05,
      "loss": 3.2633,
      "step": 100
    },
    {
      "epoch": 0.06257110352673492,
      "grad_norm": 1.4295926094055176,
      "learning_rate": 4.895714827455442e-05,
      "loss": 3.2033,
      "step": 110
    },
    {
      "epoch": 0.06825938566552901,
      "grad_norm": 1.1814738512039185,
      "learning_rate": 4.886234357224119e-05,
      "loss": 3.1245,
      "step": 120
    },
    {
      "epoch": 0.07394766780432309,
      "grad_norm": 1.2840455770492554,
      "learning_rate": 4.876753886992795e-05,
      "loss": 2.9629,
      "step": 130
    },
    {
      "epoch": 0.07963594994311718,
      "grad_norm": 1.1700204610824585,
      "learning_rate": 4.8672734167614714e-05,
      "loss": 3.2956,
      "step": 140
    },
    {
      "epoch": 0.08532423208191127,
      "grad_norm": 1.3060708045959473,
      "learning_rate": 4.8577929465301484e-05,
      "loss": 2.8537,
      "step": 150
    },
    {
      "epoch": 0.09101251422070535,
      "grad_norm": 1.465187668800354,
      "learning_rate": 4.848312476298825e-05,
      "loss": 3.0538,
      "step": 160
    },
    {
      "epoch": 0.09670079635949944,
      "grad_norm": 1.3654285669326782,
      "learning_rate": 4.838832006067501e-05,
      "loss": 3.3101,
      "step": 170
    },
    {
      "epoch": 0.10238907849829351,
      "grad_norm": 1.361220359802246,
      "learning_rate": 4.829351535836178e-05,
      "loss": 2.9929,
      "step": 180
    },
    {
      "epoch": 0.1080773606370876,
      "grad_norm": 1.4478404521942139,
      "learning_rate": 4.819871065604854e-05,
      "loss": 2.963,
      "step": 190
    },
    {
      "epoch": 0.11376564277588168,
      "grad_norm": 1.507630705833435,
      "learning_rate": 4.8103905953735306e-05,
      "loss": 3.0085,
      "step": 200
    },
    {
      "epoch": 0.11945392491467577,
      "grad_norm": 1.2725622653961182,
      "learning_rate": 4.8009101251422076e-05,
      "loss": 2.9325,
      "step": 210
    },
    {
      "epoch": 0.12514220705346984,
      "grad_norm": 1.3288112878799438,
      "learning_rate": 4.791429654910884e-05,
      "loss": 2.9901,
      "step": 220
    },
    {
      "epoch": 0.13083048919226395,
      "grad_norm": 1.39552903175354,
      "learning_rate": 4.78194918467956e-05,
      "loss": 2.8743,
      "step": 230
    },
    {
      "epoch": 0.13651877133105803,
      "grad_norm": 1.4048992395401,
      "learning_rate": 4.7724687144482366e-05,
      "loss": 2.8956,
      "step": 240
    },
    {
      "epoch": 0.1422070534698521,
      "grad_norm": 1.430892825126648,
      "learning_rate": 4.7629882442169135e-05,
      "loss": 2.937,
      "step": 250
    },
    {
      "epoch": 0.14789533560864618,
      "grad_norm": 1.3461686372756958,
      "learning_rate": 4.75350777398559e-05,
      "loss": 3.1078,
      "step": 260
    },
    {
      "epoch": 0.15358361774744028,
      "grad_norm": 1.4490413665771484,
      "learning_rate": 4.744027303754267e-05,
      "loss": 2.8674,
      "step": 270
    },
    {
      "epoch": 0.15927189988623436,
      "grad_norm": 1.6700984239578247,
      "learning_rate": 4.734546833522943e-05,
      "loss": 3.0276,
      "step": 280
    },
    {
      "epoch": 0.16496018202502843,
      "grad_norm": 1.4937001466751099,
      "learning_rate": 4.7250663632916195e-05,
      "loss": 2.9627,
      "step": 290
    },
    {
      "epoch": 0.17064846416382254,
      "grad_norm": 1.4727190732955933,
      "learning_rate": 4.715585893060296e-05,
      "loss": 2.8063,
      "step": 300
    },
    {
      "epoch": 0.17633674630261661,
      "grad_norm": 1.3600003719329834,
      "learning_rate": 4.706105422828973e-05,
      "loss": 2.6026,
      "step": 310
    },
    {
      "epoch": 0.1820250284414107,
      "grad_norm": 1.5846573114395142,
      "learning_rate": 4.696624952597649e-05,
      "loss": 2.6471,
      "step": 320
    },
    {
      "epoch": 0.18771331058020477,
      "grad_norm": 1.605607509613037,
      "learning_rate": 4.6871444823663254e-05,
      "loss": 2.7449,
      "step": 330
    },
    {
      "epoch": 0.19340159271899887,
      "grad_norm": 1.6549433469772339,
      "learning_rate": 4.6776640121350024e-05,
      "loss": 2.7424,
      "step": 340
    },
    {
      "epoch": 0.19908987485779295,
      "grad_norm": 1.4639432430267334,
      "learning_rate": 4.668183541903679e-05,
      "loss": 2.4782,
      "step": 350
    },
    {
      "epoch": 0.20477815699658702,
      "grad_norm": 1.5364162921905518,
      "learning_rate": 4.658703071672355e-05,
      "loss": 2.8291,
      "step": 360
    },
    {
      "epoch": 0.21046643913538113,
      "grad_norm": 1.5868504047393799,
      "learning_rate": 4.649222601441032e-05,
      "loss": 2.7958,
      "step": 370
    },
    {
      "epoch": 0.2161547212741752,
      "grad_norm": 1.955855131149292,
      "learning_rate": 4.6397421312097084e-05,
      "loss": 2.9948,
      "step": 380
    },
    {
      "epoch": 0.22184300341296928,
      "grad_norm": 1.6161415576934814,
      "learning_rate": 4.630261660978385e-05,
      "loss": 2.807,
      "step": 390
    },
    {
      "epoch": 0.22753128555176336,
      "grad_norm": 1.718761920928955,
      "learning_rate": 4.620781190747061e-05,
      "loss": 2.7385,
      "step": 400
    },
    {
      "epoch": 0.23321956769055746,
      "grad_norm": 1.5597854852676392,
      "learning_rate": 4.611300720515738e-05,
      "loss": 2.7961,
      "step": 410
    },
    {
      "epoch": 0.23890784982935154,
      "grad_norm": 1.6261762380599976,
      "learning_rate": 4.601820250284414e-05,
      "loss": 3.0245,
      "step": 420
    },
    {
      "epoch": 0.2445961319681456,
      "grad_norm": 1.7425310611724854,
      "learning_rate": 4.592339780053091e-05,
      "loss": 2.9694,
      "step": 430
    },
    {
      "epoch": 0.2502844141069397,
      "grad_norm": 1.6412144899368286,
      "learning_rate": 4.5828593098217676e-05,
      "loss": 2.6596,
      "step": 440
    },
    {
      "epoch": 0.25597269624573377,
      "grad_norm": 1.6944148540496826,
      "learning_rate": 4.573378839590444e-05,
      "loss": 2.5412,
      "step": 450
    },
    {
      "epoch": 0.2616609783845279,
      "grad_norm": 1.6538231372833252,
      "learning_rate": 4.56389836935912e-05,
      "loss": 2.5387,
      "step": 460
    },
    {
      "epoch": 0.267349260523322,
      "grad_norm": 1.5119332075119019,
      "learning_rate": 4.554417899127797e-05,
      "loss": 2.8844,
      "step": 470
    },
    {
      "epoch": 0.27303754266211605,
      "grad_norm": 1.8447022438049316,
      "learning_rate": 4.5449374288964735e-05,
      "loss": 2.8867,
      "step": 480
    },
    {
      "epoch": 0.2787258248009101,
      "grad_norm": 1.398255705833435,
      "learning_rate": 4.53545695866515e-05,
      "loss": 2.718,
      "step": 490
    },
    {
      "epoch": 0.2844141069397042,
      "grad_norm": 1.6721446514129639,
      "learning_rate": 4.525976488433826e-05,
      "loss": 2.8587,
      "step": 500
    },
    {
      "epoch": 0.2901023890784983,
      "grad_norm": 1.638363242149353,
      "learning_rate": 4.516496018202503e-05,
      "loss": 2.8962,
      "step": 510
    },
    {
      "epoch": 0.29579067121729236,
      "grad_norm": 1.9011434316635132,
      "learning_rate": 4.5070155479711795e-05,
      "loss": 2.7146,
      "step": 520
    },
    {
      "epoch": 0.3014789533560865,
      "grad_norm": 1.7658836841583252,
      "learning_rate": 4.4975350777398565e-05,
      "loss": 2.609,
      "step": 530
    },
    {
      "epoch": 0.30716723549488056,
      "grad_norm": 1.7144818305969238,
      "learning_rate": 4.488054607508533e-05,
      "loss": 2.711,
      "step": 540
    },
    {
      "epoch": 0.31285551763367464,
      "grad_norm": 1.695155143737793,
      "learning_rate": 4.478574137277209e-05,
      "loss": 2.5638,
      "step": 550
    },
    {
      "epoch": 0.3185437997724687,
      "grad_norm": 1.5457080602645874,
      "learning_rate": 4.4690936670458854e-05,
      "loss": 2.6847,
      "step": 560
    },
    {
      "epoch": 0.3242320819112628,
      "grad_norm": 1.672538161277771,
      "learning_rate": 4.4596131968145624e-05,
      "loss": 2.6473,
      "step": 570
    },
    {
      "epoch": 0.32992036405005687,
      "grad_norm": 1.5833854675292969,
      "learning_rate": 4.450132726583239e-05,
      "loss": 2.645,
      "step": 580
    },
    {
      "epoch": 0.33560864618885095,
      "grad_norm": 1.7899351119995117,
      "learning_rate": 4.440652256351916e-05,
      "loss": 2.7977,
      "step": 590
    },
    {
      "epoch": 0.3412969283276451,
      "grad_norm": 1.5812206268310547,
      "learning_rate": 4.431171786120592e-05,
      "loss": 2.4875,
      "step": 600
    },
    {
      "epoch": 0.34698521046643915,
      "grad_norm": 1.52481210231781,
      "learning_rate": 4.4216913158892684e-05,
      "loss": 2.5622,
      "step": 610
    },
    {
      "epoch": 0.35267349260523323,
      "grad_norm": 1.5895038843154907,
      "learning_rate": 4.412210845657945e-05,
      "loss": 2.5312,
      "step": 620
    },
    {
      "epoch": 0.3583617747440273,
      "grad_norm": 1.4821257591247559,
      "learning_rate": 4.402730375426622e-05,
      "loss": 2.9617,
      "step": 630
    },
    {
      "epoch": 0.3640500568828214,
      "grad_norm": 1.8058371543884277,
      "learning_rate": 4.393249905195298e-05,
      "loss": 2.5655,
      "step": 640
    },
    {
      "epoch": 0.36973833902161546,
      "grad_norm": 1.7320345640182495,
      "learning_rate": 4.383769434963974e-05,
      "loss": 2.6786,
      "step": 650
    },
    {
      "epoch": 0.37542662116040953,
      "grad_norm": 1.8239275217056274,
      "learning_rate": 4.3742889647326506e-05,
      "loss": 2.6282,
      "step": 660
    },
    {
      "epoch": 0.38111490329920367,
      "grad_norm": 1.6258221864700317,
      "learning_rate": 4.3648084945013276e-05,
      "loss": 2.4673,
      "step": 670
    },
    {
      "epoch": 0.38680318543799774,
      "grad_norm": 1.7344262599945068,
      "learning_rate": 4.355328024270004e-05,
      "loss": 2.6889,
      "step": 680
    },
    {
      "epoch": 0.3924914675767918,
      "grad_norm": 1.7352560758590698,
      "learning_rate": 4.345847554038681e-05,
      "loss": 2.6415,
      "step": 690
    },
    {
      "epoch": 0.3981797497155859,
      "grad_norm": 1.6515235900878906,
      "learning_rate": 4.336367083807357e-05,
      "loss": 2.6934,
      "step": 700
    },
    {
      "epoch": 0.40386803185437997,
      "grad_norm": 1.7131084203720093,
      "learning_rate": 4.3268866135760335e-05,
      "loss": 2.5069,
      "step": 710
    },
    {
      "epoch": 0.40955631399317405,
      "grad_norm": 2.0305275917053223,
      "learning_rate": 4.31740614334471e-05,
      "loss": 2.604,
      "step": 720
    },
    {
      "epoch": 0.4152445961319681,
      "grad_norm": 4.964780330657959,
      "learning_rate": 4.307925673113387e-05,
      "loss": 2.9609,
      "step": 730
    },
    {
      "epoch": 0.42093287827076226,
      "grad_norm": 1.7239235639572144,
      "learning_rate": 4.298445202882063e-05,
      "loss": 2.993,
      "step": 740
    },
    {
      "epoch": 0.42662116040955633,
      "grad_norm": 1.4889949560165405,
      "learning_rate": 4.28896473265074e-05,
      "loss": 2.5141,
      "step": 750
    },
    {
      "epoch": 0.4323094425483504,
      "grad_norm": 1.940831184387207,
      "learning_rate": 4.2794842624194165e-05,
      "loss": 2.8515,
      "step": 760
    },
    {
      "epoch": 0.4379977246871445,
      "grad_norm": 1.6206830739974976,
      "learning_rate": 4.270003792188093e-05,
      "loss": 2.7304,
      "step": 770
    },
    {
      "epoch": 0.44368600682593856,
      "grad_norm": 1.480495572090149,
      "learning_rate": 4.260523321956769e-05,
      "loss": 2.5623,
      "step": 780
    },
    {
      "epoch": 0.44937428896473264,
      "grad_norm": 1.7047971487045288,
      "learning_rate": 4.251042851725446e-05,
      "loss": 2.5435,
      "step": 790
    },
    {
      "epoch": 0.4550625711035267,
      "grad_norm": 1.8817979097366333,
      "learning_rate": 4.2415623814941224e-05,
      "loss": 2.6949,
      "step": 800
    },
    {
      "epoch": 0.46075085324232085,
      "grad_norm": 1.8404712677001953,
      "learning_rate": 4.2320819112627994e-05,
      "loss": 2.4677,
      "step": 810
    },
    {
      "epoch": 0.4664391353811149,
      "grad_norm": 1.7871347665786743,
      "learning_rate": 4.222601441031475e-05,
      "loss": 2.8546,
      "step": 820
    },
    {
      "epoch": 0.472127417519909,
      "grad_norm": 1.636745572090149,
      "learning_rate": 4.213120970800152e-05,
      "loss": 2.6887,
      "step": 830
    },
    {
      "epoch": 0.4778156996587031,
      "grad_norm": 1.7996717691421509,
      "learning_rate": 4.2036405005688284e-05,
      "loss": 2.581,
      "step": 840
    },
    {
      "epoch": 0.48350398179749715,
      "grad_norm": 1.8387221097946167,
      "learning_rate": 4.1941600303375053e-05,
      "loss": 2.9209,
      "step": 850
    },
    {
      "epoch": 0.4891922639362912,
      "grad_norm": 1.9100065231323242,
      "learning_rate": 4.184679560106182e-05,
      "loss": 2.4763,
      "step": 860
    },
    {
      "epoch": 0.4948805460750853,
      "grad_norm": 1.6824121475219727,
      "learning_rate": 4.175199089874858e-05,
      "loss": 2.8989,
      "step": 870
    },
    {
      "epoch": 0.5005688282138794,
      "grad_norm": 1.6961688995361328,
      "learning_rate": 4.165718619643534e-05,
      "loss": 2.6392,
      "step": 880
    },
    {
      "epoch": 0.5062571103526735,
      "grad_norm": 1.7537975311279297,
      "learning_rate": 4.156238149412211e-05,
      "loss": 2.662,
      "step": 890
    },
    {
      "epoch": 0.5119453924914675,
      "grad_norm": 1.7135173082351685,
      "learning_rate": 4.1467576791808876e-05,
      "loss": 2.7814,
      "step": 900
    },
    {
      "epoch": 0.5176336746302617,
      "grad_norm": 1.93651282787323,
      "learning_rate": 4.1372772089495646e-05,
      "loss": 2.5821,
      "step": 910
    },
    {
      "epoch": 0.5233219567690558,
      "grad_norm": 1.6874661445617676,
      "learning_rate": 4.12779673871824e-05,
      "loss": 2.6731,
      "step": 920
    },
    {
      "epoch": 0.5290102389078498,
      "grad_norm": 1.9278578758239746,
      "learning_rate": 4.118316268486917e-05,
      "loss": 2.4643,
      "step": 930
    },
    {
      "epoch": 0.534698521046644,
      "grad_norm": 2.033489465713501,
      "learning_rate": 4.1088357982555935e-05,
      "loss": 2.6064,
      "step": 940
    },
    {
      "epoch": 0.540386803185438,
      "grad_norm": 1.6602821350097656,
      "learning_rate": 4.0993553280242705e-05,
      "loss": 2.6248,
      "step": 950
    },
    {
      "epoch": 0.5460750853242321,
      "grad_norm": 1.8129944801330566,
      "learning_rate": 4.089874857792947e-05,
      "loss": 2.6789,
      "step": 960
    },
    {
      "epoch": 0.5517633674630261,
      "grad_norm": 1.7531323432922363,
      "learning_rate": 4.080394387561624e-05,
      "loss": 2.6559,
      "step": 970
    },
    {
      "epoch": 0.5574516496018203,
      "grad_norm": 1.7522550821304321,
      "learning_rate": 4.0709139173302995e-05,
      "loss": 2.6144,
      "step": 980
    },
    {
      "epoch": 0.5631399317406144,
      "grad_norm": 1.7209596633911133,
      "learning_rate": 4.0614334470989765e-05,
      "loss": 2.456,
      "step": 990
    },
    {
      "epoch": 0.5688282138794084,
      "grad_norm": 1.9101529121398926,
      "learning_rate": 4.051952976867653e-05,
      "loss": 2.5947,
      "step": 1000
    },
    {
      "epoch": 0.5745164960182025,
      "grad_norm": 1.9122183322906494,
      "learning_rate": 4.04247250663633e-05,
      "loss": 2.7778,
      "step": 1010
    },
    {
      "epoch": 0.5802047781569966,
      "grad_norm": 1.9505305290222168,
      "learning_rate": 4.032992036405006e-05,
      "loss": 2.6444,
      "step": 1020
    },
    {
      "epoch": 0.5858930602957907,
      "grad_norm": 1.8624544143676758,
      "learning_rate": 4.0235115661736824e-05,
      "loss": 2.6672,
      "step": 1030
    },
    {
      "epoch": 0.5915813424345847,
      "grad_norm": 1.763140082359314,
      "learning_rate": 4.014031095942359e-05,
      "loss": 2.4806,
      "step": 1040
    },
    {
      "epoch": 0.5972696245733788,
      "grad_norm": 1.7116060256958008,
      "learning_rate": 4.004550625711036e-05,
      "loss": 2.7785,
      "step": 1050
    },
    {
      "epoch": 0.602957906712173,
      "grad_norm": 1.7877191305160522,
      "learning_rate": 3.995070155479712e-05,
      "loss": 2.5715,
      "step": 1060
    },
    {
      "epoch": 0.608646188850967,
      "grad_norm": 1.8293726444244385,
      "learning_rate": 3.985589685248389e-05,
      "loss": 2.434,
      "step": 1070
    },
    {
      "epoch": 0.6143344709897611,
      "grad_norm": 2.2948577404022217,
      "learning_rate": 3.976109215017065e-05,
      "loss": 2.5195,
      "step": 1080
    },
    {
      "epoch": 0.6200227531285551,
      "grad_norm": 1.8122014999389648,
      "learning_rate": 3.966628744785742e-05,
      "loss": 2.4367,
      "step": 1090
    },
    {
      "epoch": 0.6257110352673493,
      "grad_norm": 1.742236852645874,
      "learning_rate": 3.957148274554418e-05,
      "loss": 2.6192,
      "step": 1100
    },
    {
      "epoch": 0.6313993174061433,
      "grad_norm": 2.0351133346557617,
      "learning_rate": 3.947667804323095e-05,
      "loss": 2.3937,
      "step": 1110
    },
    {
      "epoch": 0.6370875995449374,
      "grad_norm": 1.8651484251022339,
      "learning_rate": 3.938187334091771e-05,
      "loss": 2.6671,
      "step": 1120
    },
    {
      "epoch": 0.6427758816837316,
      "grad_norm": 1.8781979084014893,
      "learning_rate": 3.9287068638604476e-05,
      "loss": 2.6756,
      "step": 1130
    },
    {
      "epoch": 0.6484641638225256,
      "grad_norm": 2.0190937519073486,
      "learning_rate": 3.919226393629124e-05,
      "loss": 2.5421,
      "step": 1140
    },
    {
      "epoch": 0.6541524459613197,
      "grad_norm": 1.8050549030303955,
      "learning_rate": 3.909745923397801e-05,
      "loss": 2.6385,
      "step": 1150
    },
    {
      "epoch": 0.6598407281001137,
      "grad_norm": 2.2315075397491455,
      "learning_rate": 3.900265453166477e-05,
      "loss": 2.5708,
      "step": 1160
    },
    {
      "epoch": 0.6655290102389079,
      "grad_norm": 1.8540414571762085,
      "learning_rate": 3.890784982935154e-05,
      "loss": 2.435,
      "step": 1170
    },
    {
      "epoch": 0.6712172923777019,
      "grad_norm": 1.91741943359375,
      "learning_rate": 3.8813045127038305e-05,
      "loss": 2.6624,
      "step": 1180
    },
    {
      "epoch": 0.676905574516496,
      "grad_norm": 2.1156845092773438,
      "learning_rate": 3.871824042472507e-05,
      "loss": 2.5729,
      "step": 1190
    },
    {
      "epoch": 0.6825938566552902,
      "grad_norm": 1.7722679376602173,
      "learning_rate": 3.862343572241183e-05,
      "loss": 2.6359,
      "step": 1200
    },
    {
      "epoch": 0.6882821387940842,
      "grad_norm": 1.753456473350525,
      "learning_rate": 3.85286310200986e-05,
      "loss": 2.7055,
      "step": 1210
    },
    {
      "epoch": 0.6939704209328783,
      "grad_norm": 2.026064395904541,
      "learning_rate": 3.8433826317785365e-05,
      "loss": 3.0982,
      "step": 1220
    },
    {
      "epoch": 0.6996587030716723,
      "grad_norm": 1.8876149654388428,
      "learning_rate": 3.8339021615472135e-05,
      "loss": 2.6649,
      "step": 1230
    },
    {
      "epoch": 0.7053469852104665,
      "grad_norm": 1.433695912361145,
      "learning_rate": 3.824421691315889e-05,
      "loss": 2.6315,
      "step": 1240
    },
    {
      "epoch": 0.7110352673492605,
      "grad_norm": 1.8294188976287842,
      "learning_rate": 3.814941221084566e-05,
      "loss": 2.3983,
      "step": 1250
    },
    {
      "epoch": 0.7167235494880546,
      "grad_norm": 1.8802483081817627,
      "learning_rate": 3.8054607508532424e-05,
      "loss": 2.5432,
      "step": 1260
    },
    {
      "epoch": 0.7224118316268487,
      "grad_norm": 1.991202473640442,
      "learning_rate": 3.7959802806219194e-05,
      "loss": 2.6204,
      "step": 1270
    },
    {
      "epoch": 0.7281001137656428,
      "grad_norm": 1.7331645488739014,
      "learning_rate": 3.786499810390596e-05,
      "loss": 2.5238,
      "step": 1280
    },
    {
      "epoch": 0.7337883959044369,
      "grad_norm": 2.018613815307617,
      "learning_rate": 3.777019340159272e-05,
      "loss": 2.3684,
      "step": 1290
    },
    {
      "epoch": 0.7394766780432309,
      "grad_norm": 1.7485496997833252,
      "learning_rate": 3.7675388699279484e-05,
      "loss": 2.6712,
      "step": 1300
    },
    {
      "epoch": 0.745164960182025,
      "grad_norm": 1.8774378299713135,
      "learning_rate": 3.7580583996966253e-05,
      "loss": 2.4982,
      "step": 1310
    },
    {
      "epoch": 0.7508532423208191,
      "grad_norm": 1.839454174041748,
      "learning_rate": 3.7485779294653017e-05,
      "loss": 2.6664,
      "step": 1320
    },
    {
      "epoch": 0.7565415244596132,
      "grad_norm": 1.7456144094467163,
      "learning_rate": 3.7390974592339787e-05,
      "loss": 2.6015,
      "step": 1330
    },
    {
      "epoch": 0.7622298065984073,
      "grad_norm": 1.8833580017089844,
      "learning_rate": 3.729616989002654e-05,
      "loss": 2.4818,
      "step": 1340
    },
    {
      "epoch": 0.7679180887372014,
      "grad_norm": 2.1328296661376953,
      "learning_rate": 3.720136518771331e-05,
      "loss": 2.2225,
      "step": 1350
    },
    {
      "epoch": 0.7736063708759955,
      "grad_norm": 1.946182370185852,
      "learning_rate": 3.7106560485400076e-05,
      "loss": 2.4492,
      "step": 1360
    },
    {
      "epoch": 0.7792946530147895,
      "grad_norm": 1.7089473009109497,
      "learning_rate": 3.7011755783086846e-05,
      "loss": 2.6642,
      "step": 1370
    },
    {
      "epoch": 0.7849829351535836,
      "grad_norm": 1.9546905755996704,
      "learning_rate": 3.691695108077361e-05,
      "loss": 2.5189,
      "step": 1380
    },
    {
      "epoch": 0.7906712172923777,
      "grad_norm": 1.945075511932373,
      "learning_rate": 3.682214637846038e-05,
      "loss": 2.6082,
      "step": 1390
    },
    {
      "epoch": 0.7963594994311718,
      "grad_norm": 2.08254075050354,
      "learning_rate": 3.6727341676147135e-05,
      "loss": 2.5183,
      "step": 1400
    },
    {
      "epoch": 0.8020477815699659,
      "grad_norm": 1.7358146905899048,
      "learning_rate": 3.6632536973833905e-05,
      "loss": 2.3719,
      "step": 1410
    },
    {
      "epoch": 0.8077360637087599,
      "grad_norm": 2.394360303878784,
      "learning_rate": 3.653773227152067e-05,
      "loss": 2.6006,
      "step": 1420
    },
    {
      "epoch": 0.8134243458475541,
      "grad_norm": 2.1957759857177734,
      "learning_rate": 3.644292756920744e-05,
      "loss": 2.7562,
      "step": 1430
    },
    {
      "epoch": 0.8191126279863481,
      "grad_norm": 1.7685041427612305,
      "learning_rate": 3.63481228668942e-05,
      "loss": 2.2255,
      "step": 1440
    },
    {
      "epoch": 0.8248009101251422,
      "grad_norm": 1.9511439800262451,
      "learning_rate": 3.6253318164580965e-05,
      "loss": 2.5052,
      "step": 1450
    },
    {
      "epoch": 0.8304891922639362,
      "grad_norm": 1.850136399269104,
      "learning_rate": 3.615851346226773e-05,
      "loss": 2.4598,
      "step": 1460
    },
    {
      "epoch": 0.8361774744027304,
      "grad_norm": 2.1229028701782227,
      "learning_rate": 3.60637087599545e-05,
      "loss": 2.6601,
      "step": 1470
    },
    {
      "epoch": 0.8418657565415245,
      "grad_norm": 2.146454334259033,
      "learning_rate": 3.596890405764126e-05,
      "loss": 2.6687,
      "step": 1480
    },
    {
      "epoch": 0.8475540386803185,
      "grad_norm": 2.2313992977142334,
      "learning_rate": 3.587409935532803e-05,
      "loss": 2.6986,
      "step": 1490
    },
    {
      "epoch": 0.8532423208191127,
      "grad_norm": 1.8351048231124878,
      "learning_rate": 3.577929465301479e-05,
      "loss": 2.7693,
      "step": 1500
    },
    {
      "epoch": 0.8589306029579067,
      "grad_norm": 2.0515499114990234,
      "learning_rate": 3.568448995070156e-05,
      "loss": 2.4373,
      "step": 1510
    },
    {
      "epoch": 0.8646188850967008,
      "grad_norm": 1.9850878715515137,
      "learning_rate": 3.558968524838832e-05,
      "loss": 2.4065,
      "step": 1520
    },
    {
      "epoch": 0.8703071672354948,
      "grad_norm": 1.9145864248275757,
      "learning_rate": 3.549488054607509e-05,
      "loss": 2.5829,
      "step": 1530
    },
    {
      "epoch": 0.875995449374289,
      "grad_norm": 1.978048324584961,
      "learning_rate": 3.5400075843761853e-05,
      "loss": 2.5836,
      "step": 1540
    },
    {
      "epoch": 0.8816837315130831,
      "grad_norm": 1.8269827365875244,
      "learning_rate": 3.5305271141448617e-05,
      "loss": 2.4084,
      "step": 1550
    },
    {
      "epoch": 0.8873720136518771,
      "grad_norm": 1.7206259965896606,
      "learning_rate": 3.521046643913538e-05,
      "loss": 2.6275,
      "step": 1560
    },
    {
      "epoch": 0.8930602957906713,
      "grad_norm": 1.9114774465560913,
      "learning_rate": 3.511566173682215e-05,
      "loss": 2.5462,
      "step": 1570
    },
    {
      "epoch": 0.8987485779294653,
      "grad_norm": 2.02711820602417,
      "learning_rate": 3.502085703450891e-05,
      "loss": 2.5333,
      "step": 1580
    },
    {
      "epoch": 0.9044368600682594,
      "grad_norm": 2.0332956314086914,
      "learning_rate": 3.492605233219568e-05,
      "loss": 3.029,
      "step": 1590
    },
    {
      "epoch": 0.9101251422070534,
      "grad_norm": 1.9734388589859009,
      "learning_rate": 3.4831247629882446e-05,
      "loss": 2.4359,
      "step": 1600
    },
    {
      "epoch": 0.9158134243458476,
      "grad_norm": 1.786482334136963,
      "learning_rate": 3.473644292756921e-05,
      "loss": 2.4943,
      "step": 1610
    },
    {
      "epoch": 0.9215017064846417,
      "grad_norm": 1.8860944509506226,
      "learning_rate": 3.464163822525597e-05,
      "loss": 2.6086,
      "step": 1620
    },
    {
      "epoch": 0.9271899886234357,
      "grad_norm": 2.1260323524475098,
      "learning_rate": 3.454683352294274e-05,
      "loss": 2.5186,
      "step": 1630
    },
    {
      "epoch": 0.9328782707622298,
      "grad_norm": 2.037703275680542,
      "learning_rate": 3.4452028820629505e-05,
      "loss": 2.5173,
      "step": 1640
    },
    {
      "epoch": 0.9385665529010239,
      "grad_norm": 1.8920955657958984,
      "learning_rate": 3.4357224118316275e-05,
      "loss": 2.4857,
      "step": 1650
    },
    {
      "epoch": 0.944254835039818,
      "grad_norm": 2.0456297397613525,
      "learning_rate": 3.426241941600303e-05,
      "loss": 2.5414,
      "step": 1660
    },
    {
      "epoch": 0.949943117178612,
      "grad_norm": 1.9163354635238647,
      "learning_rate": 3.41676147136898e-05,
      "loss": 2.5019,
      "step": 1670
    },
    {
      "epoch": 0.9556313993174061,
      "grad_norm": 1.8684539794921875,
      "learning_rate": 3.4072810011376565e-05,
      "loss": 2.5456,
      "step": 1680
    },
    {
      "epoch": 0.9613196814562003,
      "grad_norm": 1.9421676397323608,
      "learning_rate": 3.3978005309063335e-05,
      "loss": 2.465,
      "step": 1690
    },
    {
      "epoch": 0.9670079635949943,
      "grad_norm": 2.090778112411499,
      "learning_rate": 3.38832006067501e-05,
      "loss": 2.6348,
      "step": 1700
    },
    {
      "epoch": 0.9726962457337884,
      "grad_norm": 2.111792802810669,
      "learning_rate": 3.378839590443686e-05,
      "loss": 2.4232,
      "step": 1710
    },
    {
      "epoch": 0.9783845278725825,
      "grad_norm": 1.8758773803710938,
      "learning_rate": 3.3693591202123624e-05,
      "loss": 2.7824,
      "step": 1720
    },
    {
      "epoch": 0.9840728100113766,
      "grad_norm": 1.8722748756408691,
      "learning_rate": 3.3598786499810394e-05,
      "loss": 2.3118,
      "step": 1730
    },
    {
      "epoch": 0.9897610921501706,
      "grad_norm": 2.03971529006958,
      "learning_rate": 3.350398179749716e-05,
      "loss": 2.8389,
      "step": 1740
    },
    {
      "epoch": 0.9954493742889647,
      "grad_norm": 2.004847764968872,
      "learning_rate": 3.340917709518393e-05,
      "loss": 2.6489,
      "step": 1750
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.7008252143859863,
      "eval_runtime": 84.9308,
      "eval_samples_per_second": 1.177,
      "eval_steps_per_second": 0.294,
      "step": 1758
    },
    {
      "epoch": 1.0011376564277588,
      "grad_norm": 1.921654224395752,
      "learning_rate": 3.3314372392870683e-05,
      "loss": 2.6077,
      "step": 1760
    },
    {
      "epoch": 1.006825938566553,
      "grad_norm": 2.000149726867676,
      "learning_rate": 3.3219567690557453e-05,
      "loss": 2.5661,
      "step": 1770
    },
    {
      "epoch": 1.012514220705347,
      "grad_norm": 1.917399287223816,
      "learning_rate": 3.3124762988244217e-05,
      "loss": 2.5883,
      "step": 1780
    },
    {
      "epoch": 1.018202502844141,
      "grad_norm": 2.0709991455078125,
      "learning_rate": 3.3029958285930987e-05,
      "loss": 2.5365,
      "step": 1790
    },
    {
      "epoch": 1.023890784982935,
      "grad_norm": 2.0033340454101562,
      "learning_rate": 3.293515358361775e-05,
      "loss": 2.5303,
      "step": 1800
    },
    {
      "epoch": 1.0295790671217293,
      "grad_norm": 2.0747768878936768,
      "learning_rate": 3.284034888130452e-05,
      "loss": 2.7546,
      "step": 1810
    },
    {
      "epoch": 1.0352673492605233,
      "grad_norm": 1.8816733360290527,
      "learning_rate": 3.2745544178991276e-05,
      "loss": 2.5458,
      "step": 1820
    },
    {
      "epoch": 1.0409556313993173,
      "grad_norm": 2.1635937690734863,
      "learning_rate": 3.2650739476678046e-05,
      "loss": 2.7708,
      "step": 1830
    },
    {
      "epoch": 1.0466439135381116,
      "grad_norm": 2.1173248291015625,
      "learning_rate": 3.255593477436481e-05,
      "loss": 2.724,
      "step": 1840
    },
    {
      "epoch": 1.0523321956769056,
      "grad_norm": 2.0753424167633057,
      "learning_rate": 3.246113007205158e-05,
      "loss": 2.7201,
      "step": 1850
    },
    {
      "epoch": 1.0580204778156996,
      "grad_norm": 1.85474693775177,
      "learning_rate": 3.236632536973834e-05,
      "loss": 2.495,
      "step": 1860
    },
    {
      "epoch": 1.0637087599544937,
      "grad_norm": 1.875768780708313,
      "learning_rate": 3.2271520667425105e-05,
      "loss": 2.5155,
      "step": 1870
    },
    {
      "epoch": 1.069397042093288,
      "grad_norm": 1.9534342288970947,
      "learning_rate": 3.217671596511187e-05,
      "loss": 2.6758,
      "step": 1880
    },
    {
      "epoch": 1.075085324232082,
      "grad_norm": 2.0668601989746094,
      "learning_rate": 3.208191126279864e-05,
      "loss": 2.7611,
      "step": 1890
    },
    {
      "epoch": 1.080773606370876,
      "grad_norm": 1.8663551807403564,
      "learning_rate": 3.19871065604854e-05,
      "loss": 2.3916,
      "step": 1900
    },
    {
      "epoch": 1.0864618885096702,
      "grad_norm": 1.8632152080535889,
      "learning_rate": 3.189230185817217e-05,
      "loss": 2.558,
      "step": 1910
    },
    {
      "epoch": 1.0921501706484642,
      "grad_norm": 2.3884663581848145,
      "learning_rate": 3.179749715585893e-05,
      "loss": 2.8792,
      "step": 1920
    },
    {
      "epoch": 1.0978384527872582,
      "grad_norm": 2.180018901824951,
      "learning_rate": 3.17026924535457e-05,
      "loss": 2.6517,
      "step": 1930
    },
    {
      "epoch": 1.1035267349260522,
      "grad_norm": 1.9802043437957764,
      "learning_rate": 3.160788775123246e-05,
      "loss": 2.465,
      "step": 1940
    },
    {
      "epoch": 1.1092150170648465,
      "grad_norm": 2.2723186016082764,
      "learning_rate": 3.151308304891923e-05,
      "loss": 2.6123,
      "step": 1950
    },
    {
      "epoch": 1.1149032992036405,
      "grad_norm": 1.8655537366867065,
      "learning_rate": 3.1418278346605994e-05,
      "loss": 2.6941,
      "step": 1960
    },
    {
      "epoch": 1.1205915813424345,
      "grad_norm": 1.990998387336731,
      "learning_rate": 3.132347364429276e-05,
      "loss": 2.6446,
      "step": 1970
    },
    {
      "epoch": 1.1262798634812285,
      "grad_norm": 1.9833807945251465,
      "learning_rate": 3.122866894197952e-05,
      "loss": 2.596,
      "step": 1980
    },
    {
      "epoch": 1.1319681456200228,
      "grad_norm": 1.994104027748108,
      "learning_rate": 3.113386423966629e-05,
      "loss": 2.5052,
      "step": 1990
    },
    {
      "epoch": 1.1376564277588168,
      "grad_norm": 2.271852970123291,
      "learning_rate": 3.1039059537353053e-05,
      "loss": 2.509,
      "step": 2000
    },
    {
      "epoch": 1.1433447098976108,
      "grad_norm": 1.8606581687927246,
      "learning_rate": 3.094425483503982e-05,
      "loss": 2.4621,
      "step": 2010
    },
    {
      "epoch": 1.149032992036405,
      "grad_norm": 2.1252670288085938,
      "learning_rate": 3.0849450132726586e-05,
      "loss": 2.4118,
      "step": 2020
    },
    {
      "epoch": 1.154721274175199,
      "grad_norm": 2.218632936477661,
      "learning_rate": 3.075464543041335e-05,
      "loss": 2.7043,
      "step": 2030
    },
    {
      "epoch": 1.1604095563139931,
      "grad_norm": 2.047490358352661,
      "learning_rate": 3.065984072810011e-05,
      "loss": 2.4059,
      "step": 2040
    },
    {
      "epoch": 1.1660978384527874,
      "grad_norm": 2.0126850605010986,
      "learning_rate": 3.056503602578688e-05,
      "loss": 2.4822,
      "step": 2050
    },
    {
      "epoch": 1.1717861205915814,
      "grad_norm": 2.4562625885009766,
      "learning_rate": 3.0470231323473646e-05,
      "loss": 2.6171,
      "step": 2060
    },
    {
      "epoch": 1.1774744027303754,
      "grad_norm": 2.2483208179473877,
      "learning_rate": 3.0375426621160412e-05,
      "loss": 2.2861,
      "step": 2070
    },
    {
      "epoch": 1.1831626848691694,
      "grad_norm": 1.7577402591705322,
      "learning_rate": 3.0280621918847176e-05,
      "loss": 2.3383,
      "step": 2080
    },
    {
      "epoch": 1.1888509670079637,
      "grad_norm": 1.9472366571426392,
      "learning_rate": 3.018581721653394e-05,
      "loss": 2.5456,
      "step": 2090
    },
    {
      "epoch": 1.1945392491467577,
      "grad_norm": 2.2227611541748047,
      "learning_rate": 3.0091012514220705e-05,
      "loss": 2.5897,
      "step": 2100
    },
    {
      "epoch": 1.2002275312855517,
      "grad_norm": 2.012181282043457,
      "learning_rate": 2.9996207811907472e-05,
      "loss": 2.4071,
      "step": 2110
    },
    {
      "epoch": 1.2059158134243457,
      "grad_norm": 1.9305213689804077,
      "learning_rate": 2.990140310959424e-05,
      "loss": 2.4674,
      "step": 2120
    },
    {
      "epoch": 1.21160409556314,
      "grad_norm": 2.2683868408203125,
      "learning_rate": 2.9806598407281e-05,
      "loss": 2.4726,
      "step": 2130
    },
    {
      "epoch": 1.217292377701934,
      "grad_norm": 2.1172900199890137,
      "learning_rate": 2.9711793704967768e-05,
      "loss": 2.5387,
      "step": 2140
    },
    {
      "epoch": 1.222980659840728,
      "grad_norm": 2.253005027770996,
      "learning_rate": 2.961698900265453e-05,
      "loss": 2.478,
      "step": 2150
    },
    {
      "epoch": 1.2286689419795223,
      "grad_norm": 1.986639142036438,
      "learning_rate": 2.9522184300341298e-05,
      "loss": 2.4653,
      "step": 2160
    },
    {
      "epoch": 1.2343572241183163,
      "grad_norm": 2.17637038230896,
      "learning_rate": 2.9427379598028064e-05,
      "loss": 2.6001,
      "step": 2170
    },
    {
      "epoch": 1.2400455062571103,
      "grad_norm": 1.8890068531036377,
      "learning_rate": 2.9332574895714827e-05,
      "loss": 2.5062,
      "step": 2180
    },
    {
      "epoch": 1.2457337883959045,
      "grad_norm": 2.1865997314453125,
      "learning_rate": 2.9237770193401594e-05,
      "loss": 2.5065,
      "step": 2190
    },
    {
      "epoch": 1.2514220705346986,
      "grad_norm": 2.1314892768859863,
      "learning_rate": 2.9142965491088357e-05,
      "loss": 2.493,
      "step": 2200
    },
    {
      "epoch": 1.2571103526734926,
      "grad_norm": 2.053083658218384,
      "learning_rate": 2.9048160788775124e-05,
      "loss": 2.6954,
      "step": 2210
    },
    {
      "epoch": 1.2627986348122868,
      "grad_norm": 2.1313109397888184,
      "learning_rate": 2.895335608646189e-05,
      "loss": 2.5633,
      "step": 2220
    },
    {
      "epoch": 1.2684869169510808,
      "grad_norm": 1.9726985692977905,
      "learning_rate": 2.8858551384148657e-05,
      "loss": 2.6777,
      "step": 2230
    },
    {
      "epoch": 1.2741751990898749,
      "grad_norm": 1.8208965063095093,
      "learning_rate": 2.876374668183542e-05,
      "loss": 2.4142,
      "step": 2240
    },
    {
      "epoch": 1.2798634812286689,
      "grad_norm": 1.8946819305419922,
      "learning_rate": 2.8668941979522183e-05,
      "loss": 2.9005,
      "step": 2250
    },
    {
      "epoch": 1.285551763367463,
      "grad_norm": 2.314253568649292,
      "learning_rate": 2.857413727720895e-05,
      "loss": 2.6438,
      "step": 2260
    },
    {
      "epoch": 1.2912400455062572,
      "grad_norm": 2.119871139526367,
      "learning_rate": 2.8479332574895716e-05,
      "loss": 2.669,
      "step": 2270
    },
    {
      "epoch": 1.2969283276450512,
      "grad_norm": 2.2832343578338623,
      "learning_rate": 2.8384527872582483e-05,
      "loss": 2.3832,
      "step": 2280
    },
    {
      "epoch": 1.3026166097838452,
      "grad_norm": 2.4994256496429443,
      "learning_rate": 2.8289723170269246e-05,
      "loss": 2.6244,
      "step": 2290
    },
    {
      "epoch": 1.3083048919226394,
      "grad_norm": 2.2780144214630127,
      "learning_rate": 2.8194918467956012e-05,
      "loss": 2.3715,
      "step": 2300
    },
    {
      "epoch": 1.3139931740614335,
      "grad_norm": 2.3156545162200928,
      "learning_rate": 2.8100113765642776e-05,
      "loss": 2.3933,
      "step": 2310
    },
    {
      "epoch": 1.3196814562002275,
      "grad_norm": 1.9844974279403687,
      "learning_rate": 2.8005309063329542e-05,
      "loss": 2.4024,
      "step": 2320
    },
    {
      "epoch": 1.3253697383390217,
      "grad_norm": 2.3630895614624023,
      "learning_rate": 2.791050436101631e-05,
      "loss": 2.6635,
      "step": 2330
    },
    {
      "epoch": 1.3310580204778157,
      "grad_norm": 2.2801592350006104,
      "learning_rate": 2.7815699658703072e-05,
      "loss": 2.3938,
      "step": 2340
    },
    {
      "epoch": 1.3367463026166098,
      "grad_norm": 2.008295774459839,
      "learning_rate": 2.772089495638984e-05,
      "loss": 2.4633,
      "step": 2350
    },
    {
      "epoch": 1.342434584755404,
      "grad_norm": 2.410959005355835,
      "learning_rate": 2.76260902540766e-05,
      "loss": 2.7195,
      "step": 2360
    },
    {
      "epoch": 1.348122866894198,
      "grad_norm": 2.1238958835601807,
      "learning_rate": 2.7531285551763368e-05,
      "loss": 2.5443,
      "step": 2370
    },
    {
      "epoch": 1.353811149032992,
      "grad_norm": 2.139451503753662,
      "learning_rate": 2.7436480849450135e-05,
      "loss": 2.5449,
      "step": 2380
    },
    {
      "epoch": 1.359499431171786,
      "grad_norm": 2.085867404937744,
      "learning_rate": 2.73416761471369e-05,
      "loss": 2.2881,
      "step": 2390
    },
    {
      "epoch": 1.36518771331058,
      "grad_norm": 1.6788400411605835,
      "learning_rate": 2.7246871444823664e-05,
      "loss": 2.4467,
      "step": 2400
    },
    {
      "epoch": 1.3708759954493743,
      "grad_norm": 2.2444984912872314,
      "learning_rate": 2.715206674251043e-05,
      "loss": 2.677,
      "step": 2410
    },
    {
      "epoch": 1.3765642775881684,
      "grad_norm": 2.0447983741760254,
      "learning_rate": 2.7057262040197194e-05,
      "loss": 2.6033,
      "step": 2420
    },
    {
      "epoch": 1.3822525597269624,
      "grad_norm": 2.1468405723571777,
      "learning_rate": 2.696245733788396e-05,
      "loss": 2.617,
      "step": 2430
    },
    {
      "epoch": 1.3879408418657566,
      "grad_norm": 2.2363502979278564,
      "learning_rate": 2.6867652635570727e-05,
      "loss": 2.5687,
      "step": 2440
    },
    {
      "epoch": 1.3936291240045506,
      "grad_norm": 2.108294725418091,
      "learning_rate": 2.677284793325749e-05,
      "loss": 2.667,
      "step": 2450
    },
    {
      "epoch": 1.3993174061433447,
      "grad_norm": 2.2791635990142822,
      "learning_rate": 2.6678043230944257e-05,
      "loss": 2.4072,
      "step": 2460
    },
    {
      "epoch": 1.405005688282139,
      "grad_norm": 1.9882280826568604,
      "learning_rate": 2.658323852863102e-05,
      "loss": 2.5168,
      "step": 2470
    },
    {
      "epoch": 1.410693970420933,
      "grad_norm": 2.099097728729248,
      "learning_rate": 2.6488433826317786e-05,
      "loss": 2.515,
      "step": 2480
    },
    {
      "epoch": 1.416382252559727,
      "grad_norm": 1.9159256219863892,
      "learning_rate": 2.6393629124004553e-05,
      "loss": 2.41,
      "step": 2490
    },
    {
      "epoch": 1.4220705346985212,
      "grad_norm": 2.133896827697754,
      "learning_rate": 2.6298824421691316e-05,
      "loss": 2.4738,
      "step": 2500
    },
    {
      "epoch": 1.4277588168373152,
      "grad_norm": 2.0079681873321533,
      "learning_rate": 2.6204019719378083e-05,
      "loss": 2.376,
      "step": 2510
    },
    {
      "epoch": 1.4334470989761092,
      "grad_norm": 2.3313474655151367,
      "learning_rate": 2.6109215017064846e-05,
      "loss": 2.7202,
      "step": 2520
    },
    {
      "epoch": 1.4391353811149032,
      "grad_norm": 1.9985945224761963,
      "learning_rate": 2.6014410314751612e-05,
      "loss": 2.3903,
      "step": 2530
    },
    {
      "epoch": 1.4448236632536973,
      "grad_norm": 2.1820180416107178,
      "learning_rate": 2.591960561243838e-05,
      "loss": 2.6022,
      "step": 2540
    },
    {
      "epoch": 1.4505119453924915,
      "grad_norm": 2.088134288787842,
      "learning_rate": 2.5824800910125142e-05,
      "loss": 2.5969,
      "step": 2550
    },
    {
      "epoch": 1.4562002275312855,
      "grad_norm": 2.1492910385131836,
      "learning_rate": 2.572999620781191e-05,
      "loss": 2.4681,
      "step": 2560
    },
    {
      "epoch": 1.4618885096700796,
      "grad_norm": 2.017002820968628,
      "learning_rate": 2.5635191505498675e-05,
      "loss": 2.4013,
      "step": 2570
    },
    {
      "epoch": 1.4675767918088738,
      "grad_norm": 2.1547999382019043,
      "learning_rate": 2.554038680318544e-05,
      "loss": 2.6089,
      "step": 2580
    },
    {
      "epoch": 1.4732650739476678,
      "grad_norm": 2.1789159774780273,
      "learning_rate": 2.5445582100872205e-05,
      "loss": 2.4689,
      "step": 2590
    },
    {
      "epoch": 1.4789533560864618,
      "grad_norm": 2.217895984649658,
      "learning_rate": 2.535077739855897e-05,
      "loss": 2.5114,
      "step": 2600
    },
    {
      "epoch": 1.484641638225256,
      "grad_norm": 2.3689332008361816,
      "learning_rate": 2.5255972696245735e-05,
      "loss": 2.5448,
      "step": 2610
    },
    {
      "epoch": 1.49032992036405,
      "grad_norm": 2.694305896759033,
      "learning_rate": 2.51611679939325e-05,
      "loss": 2.7178,
      "step": 2620
    },
    {
      "epoch": 1.4960182025028441,
      "grad_norm": 2.1524455547332764,
      "learning_rate": 2.5066363291619264e-05,
      "loss": 2.5898,
      "step": 2630
    },
    {
      "epoch": 1.5017064846416384,
      "grad_norm": 2.2168753147125244,
      "learning_rate": 2.497155858930603e-05,
      "loss": 2.3437,
      "step": 2640
    },
    {
      "epoch": 1.5073947667804322,
      "grad_norm": 2.255173444747925,
      "learning_rate": 2.4876753886992797e-05,
      "loss": 2.6126,
      "step": 2650
    },
    {
      "epoch": 1.5130830489192264,
      "grad_norm": 2.109795331954956,
      "learning_rate": 2.478194918467956e-05,
      "loss": 2.4155,
      "step": 2660
    },
    {
      "epoch": 1.5187713310580204,
      "grad_norm": 1.8669933080673218,
      "learning_rate": 2.4687144482366327e-05,
      "loss": 2.4484,
      "step": 2670
    },
    {
      "epoch": 1.5244596131968144,
      "grad_norm": 2.102987051010132,
      "learning_rate": 2.4592339780053094e-05,
      "loss": 2.7766,
      "step": 2680
    },
    {
      "epoch": 1.5301478953356087,
      "grad_norm": 2.2351627349853516,
      "learning_rate": 2.4497535077739857e-05,
      "loss": 2.4603,
      "step": 2690
    },
    {
      "epoch": 1.5358361774744027,
      "grad_norm": 2.0421674251556396,
      "learning_rate": 2.4402730375426623e-05,
      "loss": 2.4517,
      "step": 2700
    },
    {
      "epoch": 1.5415244596131967,
      "grad_norm": 2.335048198699951,
      "learning_rate": 2.4307925673113386e-05,
      "loss": 2.5893,
      "step": 2710
    },
    {
      "epoch": 1.547212741751991,
      "grad_norm": 2.0930168628692627,
      "learning_rate": 2.4213120970800153e-05,
      "loss": 2.5514,
      "step": 2720
    },
    {
      "epoch": 1.552901023890785,
      "grad_norm": 2.832164764404297,
      "learning_rate": 2.411831626848692e-05,
      "loss": 2.7223,
      "step": 2730
    },
    {
      "epoch": 1.558589306029579,
      "grad_norm": 2.3767919540405273,
      "learning_rate": 2.4023511566173683e-05,
      "loss": 2.586,
      "step": 2740
    },
    {
      "epoch": 1.5642775881683733,
      "grad_norm": 1.949428915977478,
      "learning_rate": 2.392870686386045e-05,
      "loss": 2.5553,
      "step": 2750
    },
    {
      "epoch": 1.5699658703071673,
      "grad_norm": 2.1751747131347656,
      "learning_rate": 2.3833902161547216e-05,
      "loss": 2.393,
      "step": 2760
    },
    {
      "epoch": 1.5756541524459613,
      "grad_norm": 2.0831727981567383,
      "learning_rate": 2.373909745923398e-05,
      "loss": 2.4981,
      "step": 2770
    },
    {
      "epoch": 1.5813424345847555,
      "grad_norm": 1.8842439651489258,
      "learning_rate": 2.3644292756920745e-05,
      "loss": 2.3313,
      "step": 2780
    },
    {
      "epoch": 1.5870307167235493,
      "grad_norm": 2.3480615615844727,
      "learning_rate": 2.354948805460751e-05,
      "loss": 2.5609,
      "step": 2790
    },
    {
      "epoch": 1.5927189988623436,
      "grad_norm": 2.3158535957336426,
      "learning_rate": 2.3454683352294275e-05,
      "loss": 2.6634,
      "step": 2800
    },
    {
      "epoch": 1.5984072810011376,
      "grad_norm": 2.558241128921509,
      "learning_rate": 2.3359878649981042e-05,
      "loss": 2.7315,
      "step": 2810
    },
    {
      "epoch": 1.6040955631399316,
      "grad_norm": 2.1736979484558105,
      "learning_rate": 2.3265073947667805e-05,
      "loss": 2.5106,
      "step": 2820
    },
    {
      "epoch": 1.6097838452787259,
      "grad_norm": 4.5976481437683105,
      "learning_rate": 2.317026924535457e-05,
      "loss": 2.7481,
      "step": 2830
    },
    {
      "epoch": 1.6154721274175199,
      "grad_norm": 2.4218924045562744,
      "learning_rate": 2.3075464543041338e-05,
      "loss": 2.7385,
      "step": 2840
    },
    {
      "epoch": 1.621160409556314,
      "grad_norm": 2.266512393951416,
      "learning_rate": 2.29806598407281e-05,
      "loss": 2.5462,
      "step": 2850
    },
    {
      "epoch": 1.6268486916951082,
      "grad_norm": 2.308884620666504,
      "learning_rate": 2.2885855138414868e-05,
      "loss": 2.5158,
      "step": 2860
    },
    {
      "epoch": 1.6325369738339022,
      "grad_norm": 2.7975776195526123,
      "learning_rate": 2.279105043610163e-05,
      "loss": 2.4634,
      "step": 2870
    },
    {
      "epoch": 1.6382252559726962,
      "grad_norm": 2.5691232681274414,
      "learning_rate": 2.2696245733788397e-05,
      "loss": 2.5697,
      "step": 2880
    },
    {
      "epoch": 1.6439135381114904,
      "grad_norm": 2.044532060623169,
      "learning_rate": 2.2601441031475164e-05,
      "loss": 2.4401,
      "step": 2890
    },
    {
      "epoch": 1.6496018202502845,
      "grad_norm": 2.325383186340332,
      "learning_rate": 2.2506636329161927e-05,
      "loss": 2.4566,
      "step": 2900
    },
    {
      "epoch": 1.6552901023890785,
      "grad_norm": 1.9732388257980347,
      "learning_rate": 2.2411831626848694e-05,
      "loss": 2.4953,
      "step": 2910
    },
    {
      "epoch": 1.6609783845278727,
      "grad_norm": 2.3724164962768555,
      "learning_rate": 2.231702692453546e-05,
      "loss": 2.3905,
      "step": 2920
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 2.3840737342834473,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 2.4885,
      "step": 2930
    },
    {
      "epoch": 1.6723549488054608,
      "grad_norm": 2.5796196460723877,
      "learning_rate": 2.212741751990899e-05,
      "loss": 2.7324,
      "step": 2940
    },
    {
      "epoch": 1.6780432309442548,
      "grad_norm": 2.4023830890655518,
      "learning_rate": 2.2032612817595756e-05,
      "loss": 2.3015,
      "step": 2950
    },
    {
      "epoch": 1.6837315130830488,
      "grad_norm": 2.2026820182800293,
      "learning_rate": 2.193780811528252e-05,
      "loss": 2.6171,
      "step": 2960
    },
    {
      "epoch": 1.689419795221843,
      "grad_norm": 2.027737617492676,
      "learning_rate": 2.1843003412969286e-05,
      "loss": 2.4779,
      "step": 2970
    },
    {
      "epoch": 1.695108077360637,
      "grad_norm": 2.185011863708496,
      "learning_rate": 2.174819871065605e-05,
      "loss": 2.5165,
      "step": 2980
    },
    {
      "epoch": 1.700796359499431,
      "grad_norm": 2.2719695568084717,
      "learning_rate": 2.1653394008342816e-05,
      "loss": 2.4794,
      "step": 2990
    },
    {
      "epoch": 1.7064846416382253,
      "grad_norm": 2.125476360321045,
      "learning_rate": 2.1558589306029582e-05,
      "loss": 2.6244,
      "step": 3000
    },
    {
      "epoch": 1.7121729237770194,
      "grad_norm": 2.176725387573242,
      "learning_rate": 2.1463784603716345e-05,
      "loss": 2.6688,
      "step": 3010
    },
    {
      "epoch": 1.7178612059158134,
      "grad_norm": 2.0863068103790283,
      "learning_rate": 2.1368979901403112e-05,
      "loss": 2.3271,
      "step": 3020
    },
    {
      "epoch": 1.7235494880546076,
      "grad_norm": 2.507322072982788,
      "learning_rate": 2.127417519908988e-05,
      "loss": 2.5951,
      "step": 3030
    },
    {
      "epoch": 1.7292377701934016,
      "grad_norm": 2.058633804321289,
      "learning_rate": 2.1179370496776642e-05,
      "loss": 2.4866,
      "step": 3040
    },
    {
      "epoch": 1.7349260523321957,
      "grad_norm": 2.216243028640747,
      "learning_rate": 2.1084565794463408e-05,
      "loss": 2.3988,
      "step": 3050
    },
    {
      "epoch": 1.74061433447099,
      "grad_norm": 2.2236502170562744,
      "learning_rate": 2.098976109215017e-05,
      "loss": 2.5539,
      "step": 3060
    },
    {
      "epoch": 1.7463026166097837,
      "grad_norm": 2.207960605621338,
      "learning_rate": 2.0894956389836938e-05,
      "loss": 2.4993,
      "step": 3070
    },
    {
      "epoch": 1.751990898748578,
      "grad_norm": 2.0640437602996826,
      "learning_rate": 2.0800151687523704e-05,
      "loss": 2.3138,
      "step": 3080
    },
    {
      "epoch": 1.757679180887372,
      "grad_norm": 2.0004940032958984,
      "learning_rate": 2.0705346985210468e-05,
      "loss": 2.4905,
      "step": 3090
    },
    {
      "epoch": 1.763367463026166,
      "grad_norm": 2.1705503463745117,
      "learning_rate": 2.0610542282897234e-05,
      "loss": 2.6101,
      "step": 3100
    },
    {
      "epoch": 1.7690557451649602,
      "grad_norm": 2.0164079666137695,
      "learning_rate": 2.0515737580584e-05,
      "loss": 2.5937,
      "step": 3110
    },
    {
      "epoch": 1.7747440273037542,
      "grad_norm": 2.2146828174591064,
      "learning_rate": 2.0420932878270764e-05,
      "loss": 2.6284,
      "step": 3120
    },
    {
      "epoch": 1.7804323094425483,
      "grad_norm": 2.2763659954071045,
      "learning_rate": 2.032612817595753e-05,
      "loss": 2.5696,
      "step": 3130
    },
    {
      "epoch": 1.7861205915813425,
      "grad_norm": 2.17576003074646,
      "learning_rate": 2.0231323473644294e-05,
      "loss": 2.4046,
      "step": 3140
    },
    {
      "epoch": 1.7918088737201365,
      "grad_norm": 2.7432522773742676,
      "learning_rate": 2.013651877133106e-05,
      "loss": 2.8411,
      "step": 3150
    },
    {
      "epoch": 1.7974971558589306,
      "grad_norm": 2.312882423400879,
      "learning_rate": 2.0041714069017827e-05,
      "loss": 2.5737,
      "step": 3160
    },
    {
      "epoch": 1.8031854379977248,
      "grad_norm": 2.1778905391693115,
      "learning_rate": 1.994690936670459e-05,
      "loss": 2.3112,
      "step": 3170
    },
    {
      "epoch": 1.8088737201365188,
      "grad_norm": 2.4601614475250244,
      "learning_rate": 1.9852104664391356e-05,
      "loss": 2.6564,
      "step": 3180
    },
    {
      "epoch": 1.8145620022753128,
      "grad_norm": 2.067276954650879,
      "learning_rate": 1.9757299962078123e-05,
      "loss": 2.5873,
      "step": 3190
    },
    {
      "epoch": 1.820250284414107,
      "grad_norm": 2.572950839996338,
      "learning_rate": 1.9662495259764886e-05,
      "loss": 2.6098,
      "step": 3200
    },
    {
      "epoch": 1.8259385665529009,
      "grad_norm": 2.3949084281921387,
      "learning_rate": 1.9567690557451653e-05,
      "loss": 2.5813,
      "step": 3210
    },
    {
      "epoch": 1.8316268486916951,
      "grad_norm": 2.1199164390563965,
      "learning_rate": 1.9472885855138416e-05,
      "loss": 2.5549,
      "step": 3220
    },
    {
      "epoch": 1.8373151308304891,
      "grad_norm": 2.0976150035858154,
      "learning_rate": 1.9378081152825182e-05,
      "loss": 2.5332,
      "step": 3230
    },
    {
      "epoch": 1.8430034129692832,
      "grad_norm": 2.1281096935272217,
      "learning_rate": 1.928327645051195e-05,
      "loss": 2.7119,
      "step": 3240
    },
    {
      "epoch": 1.8486916951080774,
      "grad_norm": 2.138183355331421,
      "learning_rate": 1.9188471748198712e-05,
      "loss": 2.5874,
      "step": 3250
    },
    {
      "epoch": 1.8543799772468714,
      "grad_norm": 2.4612720012664795,
      "learning_rate": 1.909366704588548e-05,
      "loss": 2.3792,
      "step": 3260
    },
    {
      "epoch": 1.8600682593856654,
      "grad_norm": 2.3656082153320312,
      "learning_rate": 1.899886234357224e-05,
      "loss": 2.7942,
      "step": 3270
    },
    {
      "epoch": 1.8657565415244597,
      "grad_norm": 2.0376272201538086,
      "learning_rate": 1.8904057641259008e-05,
      "loss": 2.546,
      "step": 3280
    },
    {
      "epoch": 1.8714448236632537,
      "grad_norm": 2.259114980697632,
      "learning_rate": 1.8809252938945775e-05,
      "loss": 2.4184,
      "step": 3290
    },
    {
      "epoch": 1.8771331058020477,
      "grad_norm": 2.1517035961151123,
      "learning_rate": 1.8714448236632538e-05,
      "loss": 2.4948,
      "step": 3300
    },
    {
      "epoch": 1.882821387940842,
      "grad_norm": 2.492504596710205,
      "learning_rate": 1.8619643534319304e-05,
      "loss": 2.4459,
      "step": 3310
    },
    {
      "epoch": 1.888509670079636,
      "grad_norm": 2.4555397033691406,
      "learning_rate": 1.852483883200607e-05,
      "loss": 2.7,
      "step": 3320
    },
    {
      "epoch": 1.89419795221843,
      "grad_norm": 2.2579662799835205,
      "learning_rate": 1.8430034129692834e-05,
      "loss": 2.7109,
      "step": 3330
    },
    {
      "epoch": 1.8998862343572243,
      "grad_norm": 2.3476595878601074,
      "learning_rate": 1.83352294273796e-05,
      "loss": 2.3472,
      "step": 3340
    },
    {
      "epoch": 1.905574516496018,
      "grad_norm": 2.331328868865967,
      "learning_rate": 1.8240424725066364e-05,
      "loss": 2.6324,
      "step": 3350
    },
    {
      "epoch": 1.9112627986348123,
      "grad_norm": 2.0996017456054688,
      "learning_rate": 1.814562002275313e-05,
      "loss": 2.4485,
      "step": 3360
    },
    {
      "epoch": 1.9169510807736063,
      "grad_norm": 2.0992014408111572,
      "learning_rate": 1.8050815320439897e-05,
      "loss": 2.6341,
      "step": 3370
    },
    {
      "epoch": 1.9226393629124003,
      "grad_norm": 2.5116233825683594,
      "learning_rate": 1.795601061812666e-05,
      "loss": 2.5321,
      "step": 3380
    },
    {
      "epoch": 1.9283276450511946,
      "grad_norm": 1.8540712594985962,
      "learning_rate": 1.7861205915813427e-05,
      "loss": 2.555,
      "step": 3390
    },
    {
      "epoch": 1.9340159271899886,
      "grad_norm": 2.1596269607543945,
      "learning_rate": 1.7766401213500193e-05,
      "loss": 2.7856,
      "step": 3400
    },
    {
      "epoch": 1.9397042093287826,
      "grad_norm": 2.1253297328948975,
      "learning_rate": 1.7671596511186956e-05,
      "loss": 2.5795,
      "step": 3410
    },
    {
      "epoch": 1.9453924914675769,
      "grad_norm": 2.035208225250244,
      "learning_rate": 1.7576791808873723e-05,
      "loss": 2.6513,
      "step": 3420
    },
    {
      "epoch": 1.9510807736063709,
      "grad_norm": 2.2332067489624023,
      "learning_rate": 1.7481987106560486e-05,
      "loss": 2.5386,
      "step": 3430
    },
    {
      "epoch": 1.956769055745165,
      "grad_norm": 2.395721435546875,
      "learning_rate": 1.7387182404247253e-05,
      "loss": 2.5283,
      "step": 3440
    },
    {
      "epoch": 1.9624573378839592,
      "grad_norm": 2.247746229171753,
      "learning_rate": 1.729237770193402e-05,
      "loss": 2.5416,
      "step": 3450
    },
    {
      "epoch": 1.9681456200227532,
      "grad_norm": 2.3899075984954834,
      "learning_rate": 1.7197572999620782e-05,
      "loss": 2.4994,
      "step": 3460
    },
    {
      "epoch": 1.9738339021615472,
      "grad_norm": 2.313809633255005,
      "learning_rate": 1.710276829730755e-05,
      "loss": 2.406,
      "step": 3470
    },
    {
      "epoch": 1.9795221843003414,
      "grad_norm": 2.639374256134033,
      "learning_rate": 1.7007963594994315e-05,
      "loss": 2.4736,
      "step": 3480
    },
    {
      "epoch": 1.9852104664391352,
      "grad_norm": 2.1510422229766846,
      "learning_rate": 1.691315889268108e-05,
      "loss": 2.4879,
      "step": 3490
    },
    {
      "epoch": 1.9908987485779295,
      "grad_norm": 2.1781930923461914,
      "learning_rate": 1.6818354190367845e-05,
      "loss": 2.4283,
      "step": 3500
    },
    {
      "epoch": 1.9965870307167235,
      "grad_norm": 2.492978811264038,
      "learning_rate": 1.6723549488054608e-05,
      "loss": 2.4568,
      "step": 3510
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.6419260501861572,
      "eval_runtime": 85.1128,
      "eval_samples_per_second": 1.175,
      "eval_steps_per_second": 0.294,
      "step": 3516
    },
    {
      "epoch": 2.0022753128555175,
      "grad_norm": 2.1430838108062744,
      "learning_rate": 1.6628744785741375e-05,
      "loss": 2.6683,
      "step": 3520
    },
    {
      "epoch": 2.0079635949943118,
      "grad_norm": 2.441383123397827,
      "learning_rate": 1.653394008342814e-05,
      "loss": 2.4298,
      "step": 3530
    },
    {
      "epoch": 2.013651877133106,
      "grad_norm": 2.3799192905426025,
      "learning_rate": 1.6439135381114904e-05,
      "loss": 2.5756,
      "step": 3540
    },
    {
      "epoch": 2.0193401592719,
      "grad_norm": 2.375096321105957,
      "learning_rate": 1.634433067880167e-05,
      "loss": 2.6446,
      "step": 3550
    },
    {
      "epoch": 2.025028441410694,
      "grad_norm": 2.362240791320801,
      "learning_rate": 1.6249525976488434e-05,
      "loss": 2.2679,
      "step": 3560
    },
    {
      "epoch": 2.030716723549488,
      "grad_norm": 2.420996904373169,
      "learning_rate": 1.61547212741752e-05,
      "loss": 2.563,
      "step": 3570
    },
    {
      "epoch": 2.036405005688282,
      "grad_norm": 2.306633949279785,
      "learning_rate": 1.6059916571861967e-05,
      "loss": 2.5894,
      "step": 3580
    },
    {
      "epoch": 2.0420932878270763,
      "grad_norm": 2.1852779388427734,
      "learning_rate": 1.596511186954873e-05,
      "loss": 2.3614,
      "step": 3590
    },
    {
      "epoch": 2.04778156996587,
      "grad_norm": 2.4933385848999023,
      "learning_rate": 1.5870307167235497e-05,
      "loss": 2.3391,
      "step": 3600
    },
    {
      "epoch": 2.0534698521046644,
      "grad_norm": 2.078113555908203,
      "learning_rate": 1.5775502464922263e-05,
      "loss": 2.5894,
      "step": 3610
    },
    {
      "epoch": 2.0591581342434586,
      "grad_norm": 2.384788751602173,
      "learning_rate": 1.5680697762609027e-05,
      "loss": 2.5969,
      "step": 3620
    },
    {
      "epoch": 2.0648464163822524,
      "grad_norm": 2.009657382965088,
      "learning_rate": 1.5585893060295793e-05,
      "loss": 2.4634,
      "step": 3630
    },
    {
      "epoch": 2.0705346985210467,
      "grad_norm": 2.5535335540771484,
      "learning_rate": 1.5491088357982556e-05,
      "loss": 2.3844,
      "step": 3640
    },
    {
      "epoch": 2.076222980659841,
      "grad_norm": 1.9684168100357056,
      "learning_rate": 1.5396283655669323e-05,
      "loss": 2.409,
      "step": 3650
    },
    {
      "epoch": 2.0819112627986347,
      "grad_norm": 2.747817277908325,
      "learning_rate": 1.530147895335609e-05,
      "loss": 2.5359,
      "step": 3660
    },
    {
      "epoch": 2.087599544937429,
      "grad_norm": 2.2008137702941895,
      "learning_rate": 1.5206674251042851e-05,
      "loss": 2.3287,
      "step": 3670
    },
    {
      "epoch": 2.093287827076223,
      "grad_norm": 2.6688344478607178,
      "learning_rate": 1.5111869548729617e-05,
      "loss": 2.4949,
      "step": 3680
    },
    {
      "epoch": 2.098976109215017,
      "grad_norm": 2.237356424331665,
      "learning_rate": 1.5017064846416384e-05,
      "loss": 2.5211,
      "step": 3690
    },
    {
      "epoch": 2.1046643913538112,
      "grad_norm": 2.285531997680664,
      "learning_rate": 1.4922260144103147e-05,
      "loss": 2.6733,
      "step": 3700
    },
    {
      "epoch": 2.110352673492605,
      "grad_norm": 2.4523653984069824,
      "learning_rate": 1.4827455441789914e-05,
      "loss": 2.4276,
      "step": 3710
    },
    {
      "epoch": 2.1160409556313993,
      "grad_norm": 2.423203945159912,
      "learning_rate": 1.4732650739476679e-05,
      "loss": 2.5844,
      "step": 3720
    },
    {
      "epoch": 2.1217292377701935,
      "grad_norm": 2.4080348014831543,
      "learning_rate": 1.4637846037163443e-05,
      "loss": 2.4565,
      "step": 3730
    },
    {
      "epoch": 2.1274175199089873,
      "grad_norm": 2.319547414779663,
      "learning_rate": 1.454304133485021e-05,
      "loss": 2.7199,
      "step": 3740
    },
    {
      "epoch": 2.1331058020477816,
      "grad_norm": 2.3682332038879395,
      "learning_rate": 1.4448236632536975e-05,
      "loss": 2.42,
      "step": 3750
    },
    {
      "epoch": 2.138794084186576,
      "grad_norm": 2.076298236846924,
      "learning_rate": 1.435343193022374e-05,
      "loss": 2.5999,
      "step": 3760
    },
    {
      "epoch": 2.1444823663253696,
      "grad_norm": 2.1413724422454834,
      "learning_rate": 1.4258627227910504e-05,
      "loss": 2.4374,
      "step": 3770
    },
    {
      "epoch": 2.150170648464164,
      "grad_norm": 2.4969944953918457,
      "learning_rate": 1.416382252559727e-05,
      "loss": 2.5562,
      "step": 3780
    },
    {
      "epoch": 2.155858930602958,
      "grad_norm": 2.2456369400024414,
      "learning_rate": 1.4069017823284036e-05,
      "loss": 2.5068,
      "step": 3790
    },
    {
      "epoch": 2.161547212741752,
      "grad_norm": 2.66288685798645,
      "learning_rate": 1.39742131209708e-05,
      "loss": 2.5394,
      "step": 3800
    },
    {
      "epoch": 2.167235494880546,
      "grad_norm": 2.495929479598999,
      "learning_rate": 1.3879408418657566e-05,
      "loss": 2.4219,
      "step": 3810
    },
    {
      "epoch": 2.1729237770193404,
      "grad_norm": 2.419503688812256,
      "learning_rate": 1.3784603716344332e-05,
      "loss": 2.5367,
      "step": 3820
    },
    {
      "epoch": 2.178612059158134,
      "grad_norm": 2.529564380645752,
      "learning_rate": 1.3689799014031097e-05,
      "loss": 2.5022,
      "step": 3830
    },
    {
      "epoch": 2.1843003412969284,
      "grad_norm": 2.385875701904297,
      "learning_rate": 1.3594994311717862e-05,
      "loss": 2.4871,
      "step": 3840
    },
    {
      "epoch": 2.189988623435722,
      "grad_norm": 2.6855340003967285,
      "learning_rate": 1.3500189609404627e-05,
      "loss": 2.7216,
      "step": 3850
    },
    {
      "epoch": 2.1956769055745164,
      "grad_norm": 2.3222134113311768,
      "learning_rate": 1.3405384907091391e-05,
      "loss": 2.5145,
      "step": 3860
    },
    {
      "epoch": 2.2013651877133107,
      "grad_norm": 2.3209335803985596,
      "learning_rate": 1.3310580204778158e-05,
      "loss": 2.3908,
      "step": 3870
    },
    {
      "epoch": 2.2070534698521045,
      "grad_norm": 2.4972622394561768,
      "learning_rate": 1.3215775502464923e-05,
      "loss": 2.8783,
      "step": 3880
    },
    {
      "epoch": 2.2127417519908987,
      "grad_norm": 2.342683792114258,
      "learning_rate": 1.3120970800151688e-05,
      "loss": 2.4146,
      "step": 3890
    },
    {
      "epoch": 2.218430034129693,
      "grad_norm": 2.6526122093200684,
      "learning_rate": 1.3026166097838454e-05,
      "loss": 2.4072,
      "step": 3900
    },
    {
      "epoch": 2.2241183162684868,
      "grad_norm": 2.437563180923462,
      "learning_rate": 1.2931361395525219e-05,
      "loss": 2.3409,
      "step": 3910
    },
    {
      "epoch": 2.229806598407281,
      "grad_norm": 2.338510036468506,
      "learning_rate": 1.2836556693211984e-05,
      "loss": 2.4995,
      "step": 3920
    },
    {
      "epoch": 2.2354948805460753,
      "grad_norm": 2.5992538928985596,
      "learning_rate": 1.2741751990898749e-05,
      "loss": 2.7806,
      "step": 3930
    },
    {
      "epoch": 2.241183162684869,
      "grad_norm": 2.2455241680145264,
      "learning_rate": 1.2646947288585514e-05,
      "loss": 2.2708,
      "step": 3940
    },
    {
      "epoch": 2.2468714448236633,
      "grad_norm": 2.3307929039001465,
      "learning_rate": 1.255214258627228e-05,
      "loss": 2.5318,
      "step": 3950
    },
    {
      "epoch": 2.252559726962457,
      "grad_norm": 2.44352650642395,
      "learning_rate": 1.2457337883959045e-05,
      "loss": 2.6275,
      "step": 3960
    },
    {
      "epoch": 2.2582480091012513,
      "grad_norm": 2.4570400714874268,
      "learning_rate": 1.236253318164581e-05,
      "loss": 2.7424,
      "step": 3970
    },
    {
      "epoch": 2.2639362912400456,
      "grad_norm": 2.705479621887207,
      "learning_rate": 1.2267728479332575e-05,
      "loss": 2.6926,
      "step": 3980
    },
    {
      "epoch": 2.26962457337884,
      "grad_norm": 2.5104024410247803,
      "learning_rate": 1.2172923777019341e-05,
      "loss": 2.4881,
      "step": 3990
    },
    {
      "epoch": 2.2753128555176336,
      "grad_norm": 2.151522397994995,
      "learning_rate": 1.2078119074706106e-05,
      "loss": 2.6498,
      "step": 4000
    }
  ],
  "logging_steps": 10,
  "max_steps": 5274,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5428043348705280.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
